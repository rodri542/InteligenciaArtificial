#+TITLE: Visión por Computadora 
#+author: Eduardo Alcaraz
#+email: eduardo.ac@morelia.tecnm.mx

* ¿Que es la visión por computadora?

La visión por computadora es un campo de la inteligencia artificial
asociado al análisis de imágenes y vídeos, que incluye un conjunto de
técnicas que otorgan a la computadora la capacidad de *ver* y extraer
información de aquello que se ha visto.

Los sistemas se componen de una cámara fotográfica o de vídeo y un
software especializado que identifica y clasifica objetos. Son capaces
de analizar imágenes (fotos, imágenes, vídeos, códigos de barras), así
como caras y emociones.

Para enseñar a una computadora a *ver* , se utilizan tecnologías de
aprendizaje automático y se recopilan muchos datos que permiten
resaltar características y combinaciones de las mismas para
identificar aún más objetos similares.


* Visión por computadora
#+startup: inlineimages
#+ATTR_LATEX: :width 0.3\textwidth
[[file:img/im1.jpg]]

* Aspectos Claves de la visión por computadora 

 - *Procesamiento de Imágenes*: Involucra técnicas para mejorar
   imágenes digitales, extraer información útil, realizar ajustes como
   corrección de color, reducción de ruido, y más.

 - *Reconocimiento de Patrones*: Se refiere a la identificación de
   patrones, formas y características en las imágenes. Esto puede
   incluir el reconocimiento de rostros, objetos, escenas, gestos,
   etc.

 - *Segmentación de Imágenes*: Se trata de dividir una imagen en
   diferentes partes o segmentos, a menudo para aislar regiones o
   elementos de interés.

 - *Detección y Seguimiento de Objetos*: Implica identificar y seguir
   objetos a lo largo del tiempo en una serie de imágenes o vídeos.

 - *Reconstrucción 3D*: Consiste en crear representaciones
   tridimensionales de un objeto o escena a partir de imágenes
   bidimensionales.

 - *Visión Artificial en Robótica*: Aplicación de la visión por
   computadora en robots para la navegación, manipulación de objetos y
   otras tareas.

 - *Interpretación de Escenas*: Comprende el análisis de escenas
   completas en términos de identificación de objetos, su disposición
   espacial, interacciones, y el contexto general.

La visión por computadora se utiliza en una variedad de aplicaciones,
que van desde la seguridad y vigilancia hasta la medicina, pasando por
los vehículos autónomos, la inspección industrial, la gestión de
contenido digital y la interacción hombre-máquina. Este campo se
beneficia enormemente de los avances en el aprendizaje automático y el
aprendizaje profundo, permitiendo el desarrollo de sistemas más
precisos y eficientes para el análisis visual.

* Libros de Visión por Computadora 
- Geometría de Vistas Múltiples en Visión por Computadora *Richard
  Hartley*
- Visión por Computadora Modelos, Aprendizaje e Inferencia *Simon
  J. D. Prince*
- Visión por Computadora Un Enfoque Moderno *David A. Forsyth*
- Aprendizaje Profundo Práctico para la Nube, Móvil y Edge Proyectos
  Reales de IA y Visión por Computadora Usando Python, Keras y
  TensorFlow *Anirudh Koul*
- Aprendiendo OpenCV 4 Visión por Computadora con Python 3 Entiende
  las herramientas, técnicas y algoritmos para la visión por
  computadora y el aprendizaje automático *Joseph Howse*

* Tecnologías y Herramientas
  - Lenguajes de Programación: Python, C++, C.
  - Bibliotecas y Frameworks: OpenCV, TensorFlow, PyTorch.

**   Instalación Python Opencv


*** Instalación python Opencv Windows


- *Paso 1: Instalar Python 3*
   - Python se puede descargar desde la página oficial: [[https://www.python.org/downloads/][Python Downloads]].
   - Asegúrate de marcar la opción "Add Python 3.x to PATH" durante la instalación.

- *Paso 2: Verificar la Instalación de Python*
   - Abre la Terminal de Comandos (Command Prompt) y ejecuta:
     #+BEGIN_SRC bash
     python --version
     #+END_SRC
   - Esto debería mostrar la versión de Python instalada.

- *Paso 3: Actualizar pip (Gestor de Paquetes de Python)*
   - En la Terminal de Comandos, ejecuta:
     #+BEGIN_SRC bash
     python -m pip install --upgrade pip
     #+END_SRC

- *Paso 4: Instalar OpenCV*
   - Utiliza pip para instalar OpenCV. En la Terminal de Comandos, ejecuta:
     #+BEGIN_SRC bash
     pip install opencv-python
     #+END_SRC
   - Si necesitas las funcionalidades adicionales de OpenCV, instala también opencv-contrib-python:
     #+BEGIN_SRC bash
     pip install opencv-contrib-python
     #+END_SRC

- *Paso 5: Verificar la Instalación de OpenCV*
   - Para verificar que OpenCV está instalado, abre un intérprete de Python y ejecuta:
     #+BEGIN_SRC python
     import cv2
     print(cv2.__version__)
     #+END_SRC
   - Si se muestra la versión de OpenCV sin errores, la instalación
     fue exitosa.

- *Notas Finales*
   - Es recomendable reiniciar el sistema después de instalar Python
     para asegurar que todos los cambios de configuración se apliquen
     correctamente.
   - Puede ser útil trabajar en un entorno virtual para proyectos de
     Python para gestionar las dependencias de manera más eficiente.

	


*** Instalación python Opencv Mac 

Instalar Python 3 y OpenCV en macOS es un proceso bastante sencillo. A
continuación, se muestran los pasos para realizar esta instalación.

- *Paso 1: Instalar Python 3*
macOS viene con Python 2.7 instalado por defecto, pero se recomienda usar Python 3 para proyectos nuevos.

   #+BEGIN_SRC bash
   /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
   brew install python3
   #+END_SRC
   Verificar la versión de Python:
   #+BEGIN_SRC bash
   python3 --version
   #+END_SRC

- *Paso 2: Instalar pip*
   pip es el gestor de paquetes de Python y generalmente viene instalado con Python 3.
   Verificar si pip está instalado:
   #+BEGIN_SRC bash
   pip3 --version
   #+END_SRC
   Si pip no está instalado:
   #+BEGIN_SRC bash
   sudo easy_install pip
   #+END_SRC

- *Paso 3: Instalar OpenCV*
   Puedes instalar OpenCV para Python utilizando pip.
   Instalación básica:
   #+BEGIN_SRC bash
   pip3 install opencv-python
   #+END_SRC
   Instalar con funcionalidades adicionales:
   #+BEGIN_SRC bash
   pip3 install opencv-contrib-python
   #+END_SRC

- *Paso 4: Verificar la Instalación de OpenCV*
   Para comprobar que OpenCV esté correctamente instalado:
   #+BEGIN_SRC python
   import cv2
   print(cv2.__version__)
   #+END_SRC

- *Notas Finales*
  - Es una buena práctica trabajar en un entorno virtual para proyectos de Python.
  - Asegúrate de que tu sistema macOS esté actualizado.





*** Instalación python Opencv Linux

Instalar Python 3 y OpenCV en un sistema Linux generalmente es un
proceso sencillo. A continuación, te presento los pasos genéricos para
la mayoría de las distribuciones de Linux. Ten en cuenta que estos
pasos pueden variar ligeramente dependiendo de la distribución
específica que estés utilizando (como Ubuntu, Fedora, etc.).

- *Instalar Python 3*

La mayoría de las distribuciones modernas de Linux ya vienen con
Python 3 instalado. Puedes verificar si Python 3 está instalado y su
versión usando el siguiente comando en la terminal:

#+BEGIN_SRC shell
python3 --version
#+END_SRC

Si Python 3 no está instalado o deseas una versión más reciente,
puedes instalarlo a través del gestor de paquetes de tu distribución:

- *En distribuciones basadas en Debian (como Ubuntu):*

  #+BEGIN_SRC shell
sudo apt update
sudo apt install python3
#+END_SRC

- *En distribuciones basadas en Red Hat (como Fedora):*

#+BEGIN_SRC shell
 sudo dnf install python3
#+END_SRC

- *Instalar pip (Gestor de Paquetes de Python)*

*pip* es el gestor de paquetes para Python y se utiliza para instalar paquetes de Python como OpenCV. Puedes instalar `pip` con el siguiente comando:

- *En Ubuntu y otras distribuciones basadas en Debian:*

#+BEGIN_SRC bash
 sudo apt install python3-pip
#+END_SRC
 
- En Fedora y distribuciones basadas en Red Hat:

  sudo dnf install python3-pip


- *Instalar OpenCV*

Una vez que tengas Python 3 y pip instalados, puedes instalar OpenCV. El paquete `opencv-python` proporciona enlaces a las bibliotecas de OpenCV y es el método más fácil de instalar OpenCV para Python. Ejecuta el siguiente comando:

#+BEGIN_SRC bash
pip3 install opencv-python
#+END_SRC

Si necesitas los módulos adicionales (que incluyen algoritmos patentados), puedes instalar `opencv-contrib-python`:

#+BEGIN_SRC bash
pip3 install opencv-contrib-python
#+END_SRC

- *Verificar la Instalación*

Para verificar que OpenCV está correctamente instalado, puedes hacer lo siguiente:

1. Abre una terminal y escribe `python3` para entrar en el intérprete interactivo de Python.

2. En el intérprete, escribe:

#+BEGIN_SRC python
 import cv2
 print(cv2.__version__)
  
#+END_SRC
 
   Si no hay errores y se muestra la versión de OpenCV, significa que la instalación fue exitosa.

- *Notas Adicionales*

   - Si estás trabajando en un entorno de desarrollo profesional o experimental, es una buena práctica usar entornos virtuales para gestionar las dependencias de Python. Puedes usar herramientas como `venv` o `conda` para crear entornos virtuales.

   - Asegúrate de que tu sistema esté actualizado antes de comenzar la instalación.

   - Los pasos exactos pueden variar ligeramente dependiendo de la versión y el tipo de tu distribución de Linux. 


* Aplicaciones de la Visión por Computadora
  - Reconocimiento Facial: Uso en seguridad y dispositivos móviles.
  - Vehículos Autónomos: Navegación y detección de obstáculos.
  - Análisis Médico de Imágenes: Aplicación en diagnóstico y análisis.

** Programación 
*** Cargar imagen 
   #+BEGIN_SRC python :results output
import cv2 as cv 
img = cv.imread('/home/likcos/Imágenes/tr.png', 0)
cv.imshow('ejemplo', img)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC
   #+RESULTS:
*** Modelos de Color

   #+BEGIN_SRC python :results output
import cv2 as cv 
img = cv.imread('/home/likcos/Imágenes/tr.png', 1)
imgGris = cv.cvtColor(img, cv.COLOR_BGR2GRAY)        
cv.imshow('ejemplo', img)
cv.imshow('ejemploGris', imgGris)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC

#+RESULTS:

*** Filtros de convolución 
#+BEGIN_SRC python
import cv2 as cv 
import numpy as np 

img = cv.imread('/home/likcos/Imágenes/mo1.png',0)
mtz = np.array([[-1,-2,-1],
                [0,0,0],
                [1,2,1]])
resultado = cv.filter2D(img, -1, mtz)
cv.imshow('marcoc', resultado)
cv.imshow('marco', img)
cv.waitKey(0)
cv.destroyAllWindows()


#+END_SRC

#+RESULTS:
: None




*** Canales de color
#+BEGIN_SRC python :results output
import cv2 as cv
import numpy as np 
img = cv.imread('img/tr.png')
img2 = np.zeros(img.shape[:2], dtype=np.uint8)
print(img.shape[1])
b,g,r =cv.split(img)
rb=cv.merge([b,img2,img2])
rg=cv.merge([img2,g,img2])
rr=cv.merge([img2,img2,r])

cv.imshow('img', img)
#cv.imshow('img2',img2)
cv.imshow('b',rb)
cv.imshow('g',rg)
cv.imshow('r',rr)
cv.waitKey(0)
cv.destroyAllWindows()


#+END_SRC

#+RESULTS:
: 635

*** Segmentación de color 

#+BEGIN_SRC python
import cv2 as cv
img = cv.imread('img/tr.png',1)
imghsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
imgRGB = cv.cvtColor(img, cv.COLOR_BGR2RGB)
ubb=(0,100, 100)
uba=(20, 255,255)
mask = cv.inRange(imghsv, ubb, uba)
res = cv.bitwise_and(img, img, mask=mask)
xcv.imshow('img', img)
cv.imshow('Resultado',res )
cv.imshow('mask', mask)
cv.waitKey(0)
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:
: None

*** Transformaciones Geométricas 

**** Transformaciones Afín 
  La transformación afín es una transformación geométrica que esta
  constituida por translación, escalamiento, rotación y
  cizallamiento. Cada una de estas Transformaciones es una
  Transformación afín. 
  
**** Traslación 
 Una translación la podemos hacer simplemente asumiendo que nuevas
 coordenadas $\hat{x} = x + t_x  \hat{y} = y + t_y$ les sumamos un valor
 $t_x$ p $t_y$ según corresponda. En coordenadas homogéneas queda como 

$$
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&\\
 1
 \end{pmatrix}
 = 
 \begin{pmatrix}
 1 & 0& t_x\\
 0 & 1 & t_y&\\
 0 & 0 & 1
 \end{pmatrix}
\begin{pmatrix}
 x\\
 y&\\
 1
 \end{pmatrix} 
$$



 #+BEGIN_SRC python
import cv2 as cv
import numpy as np
img = cv.imread('/home/likcos/Imágenes/mo1.png',0)
h,w = img.shape[:2]
img2 = np.zeros((h*2, w*2, 1) , dtype = "uint8")
print("Valores " + str(img.shape[:2]))
for i in range(h):
    for j in range(w):
        img2[int(i*0.5),int(j*0.5)]=img[i,j]

cv.imshow('imagen', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()

 #+END_SRC
  

 #+RESULTS:
 : None


**** Escalamiento 
	 El escalamiento puede entenderse como hacer una figura geométrica
	 cambie su tamaño o cambie su escala. Un escalamiento en x lo
	 podemos representar por como $\hat{x} = x$, $s_x$ y en y como
	 $\hat{y} = y$,  $s_y$ En coordenada homogéneas se puede expresar como 
$$
 \begin{pmatrix}
 \hat{xs}\\
 \hat{y}&\\
 1
 \end{pmatrix}
 = 
 \begin{pmatrix}
 s_x & 0& 0\\
 0 & s_y & 0&\\
 0 & 0 & 1
 \end{pmatrix}
 
\begin{pmatrix}
 x\\
 y&\\
 1
 \end{pmatrix}
$$	

 #+BEGIN_SRC python :results output
import cv2 as cv
import numpy as np
img = cv.imread('/home/likcos/Imágenes/mo1.png',0)
h,w = img.shape[:2]
print(h, w)
img2 = np.zeros((h*2, w*2) , dtype = "uint8")
print("Valores " + str(img.shape[:2]))
for i in range(h):
    for j in range(w):
        img2[int(i*2),int(j*2)]=img[i,j]

cv.imshow('imagen', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()

 #+END_SRC

 #+RESULTS:
 : 441 524
 : Valores (441, 524)

**** Rotación 

 Considerando el caso de un punto que rota respecto a un punto
 fijo. Las coordenadas x y y, en forma polar las podemos obtener como $x=r$ 
 $cos(\theta) y y = r sen(\theta)$. Si consideramos que esta gira un ángulo $\theta$    
 entonces podemos representar esta rotación en forma polar. 
 
$$
 \begin{equation}
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&
 \end{pmatrix}
 = 
 \begin{pmatrix}
 r cos(\alpha + \theta)\\
 r sen(\alpha + \theta)
 \end{pmatrix}
 = 
 \begin{pmatrix}
 r cos(\alpha + \theta) - r sin(\alpha) sin(\theta) \\
 r sen(\alpha + \theta) + r sin(\alpha) con(\theta)
 \end{pmatrix}
 \end{equation}


 \begin{equation}
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&
 \end{pmatrix}
 = 
 \begin{pmatrix}
 x cos(\theta) - y sin(\theta) \\
 x sen(\theta) + y cos(\theta)
 \end{pmatrix}
 \end{equation}


 \begin{equation}
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&
 \end{pmatrix}
 = 
 \begin{pmatrix}
  cos(\theta) &-  sin(\theta) \\
  sen(\theta) &  cos(\theta)
 \end{pmatrix}
 \begin{pmatrix}
 x \\
 y 
 \end{pmatrix}

 \end{equation}

 \begin{equation}
 xcos(\theta) - ysin(\theta), xsen(\theta) + ycos(\theta)
 \end{equation}
$$
 
#+BEGIN_SRC python :results output
import cv2 as cv
import math
import numpy as np 

img = cv.imread('/home/likcos/Imágenes/mo1.png',0)
h,w = img.shape[:2]
img2 = np.zeros((h*3, w*3), dtype = "uint8")
for i in range(h):
    for j in range(w):
        img2[int(i*math.cos(math.radians(30))-j*math.sin(math.radians(30)))+200,
             int(i*(math.sin(math.radians(30)))+j*math.cos(math.radians(30)))+50]=img[i,j]
cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()
 #+END_SRC

 #+RESULTS:

**** Cizallamiento 

   El cizallamiento es una transformación dada por la matriz, donde $c_x$
   es el ángulo de cizallamiento respecto al eje x

   \begin{equation}
   C_x
   = 
   \begin{pmatrix}
   1 & tg(C_x)& 0\\
   0 & 1 & 0&\\
   0 & 0 & 1
   \end{pmatrix}

   \end{equation}



   #+BEGIN_SRC python :results output
import cv2 as cv
import math
import numpy as np 

img = cv.imread('/home/likcos/Imágenes/mo1.png',0)
h,w = img.shape[:2]
img2 = np.zeros((h*2, w*2), dtype = "uint8")
matz = np.array([[1,1,1],[1,1,1],[1,1,1]])
for i in range(h):
    for j in range(w):
        img2[int(i*2) ,int(j*2)]=img[i,j]
res = cv.filter2D(img2, -1, matz)
cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.imshow('imagen3', res)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC

   #+RESULTS:

*** Traslación Opencv  WarpAffine Afine

   #+BEGIN_SRC python
import cv2 as cv
import numpy as np 

img = cv.imread('/home/likcos/Imágenes/mo1.png')
h,w = img.shape[:2]
mw = np.float32([[1,0,10],[0,1,10]])
img2 = cv.warpAffine(img,mw,(h,w))

cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()


   #+END_SRC

   #+RESULTS:
   : None

**** Rotación Opencv WarpAffine + getRotationMatrix2D

   #+BEGIN_SRC python
import cv2 as cv
import numpy as np 

img = cv.imread('/home/likcos/Imágenes/mo1.png')
h,w = img.shape[:2]

mw = cv.getRotationMatrix2D((h//2, w//2),30,-1)
img2 = cv.warpAffine(img,mw,(h,w))

cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC

   #+RESULTS:
   : None

*** Primitivas de Dibujo

   #+BEGIN_SRC python
import cv2 as cv 
import numpy as np 
img = 58*np.ones((1000,1000,3), np.uint8)
cv.line(img,(0,0), (100,100), (23, 189, 200), 3)
cv.rectangle(img, (40,40), (80,80), (1,65,90), -1)
cv.circle(img, (100,100), 50, (45, 190,200),-1)
cv.circle(img, (100,100), 45, (45, 200,90),-1)
cv.ellipse(img,(256,256),(100,50),0,0,180,255,-1)
pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)
pts = pts.reshape((-1,1,2))
cv.polylines(img,[pts],True,(0,255,255))
cv.imshow('marco',img)
cv.waitKey(0)
cv.destroyAllWindows()

   #+END_SRC

   #+RESULTS:


   #+begin_src python :results output
import cv2 as cv 
import numpy as np 
import math

Pi = 3.1416
img = 255 * np.ones((500, 500, 3 ), np.uint8)

for i in range(360):
    #img = 255 * np.ones((500, 500, 3 ), np.uint8)
    h, w = img.shape[:2] 
   
    #x = int(h/2) + int(100* math.sin(6*(i*(Pi/180))))*math.sin(i*Pi/180)
    #y = int(w/2) + int(100* math.sin(6*(i*(Pi/180))))*math.cos(i*Pi/180)
    
    #xx = int(h/3) + int(100* (-1+math.cos(i*(Pi/180)))*math.sin(i*Pi/180))
    #yy = int(w/3) + int(100* (-1+math.cos(i*(Pi/180)))*math.cos(i*Pi/180))

    xx = int(h/2) + int(100* (math.cos(1*(i*(Pi/180))))*(-1*(math.cos(80*(i*Pi/180)))))
    yy = int(w/2) + int(100* (math.sin(1*(i*(Pi/180))))*(-1*(math.sin(80*(i*Pi/180)))))

    #cv.circle(img, (int(x) , int(y)), 3, (0,i,0), -1)
    #cv.circle(img, (int(y) , int(x)), 3, (i,0,0), -1)
    cv.circle(img, (int(xx) , int(yy)), 1, (0,0,i), -1)
    #cv.imwrite('resultado'+str(i)+'.jpg',img)

    cv.imshow('imagen', img)
    cv.waitKey(10)

cv.imshow('imagen', img)
cv.imwrite('resultado.jpg',img)
cv.waitKey(0)
cv.destroyAllWindows()
   
   #+end_src



*** Flujo óptico 

El flujo óptico es un concepto en visión por computadora y
procesamiento de imágenes que se refiere al patrón de movimiento
aparente de los objetos, las superficies y los bordes en una escena
visual causado por el movimiento relativo entre un observador y la
escena. La idea es estimar cómo se mueven los puntos de una imagen
entre dos cuadros consecutivos de un video o entre dos imágenes
tomadas en momentos diferentes.

*Conceptos Clave del Flujo Óptico:* Vector de Movimiento: Cada punto en
la imagen tiene asociado un vector que indica la dirección y la
magnitud del movimiento de ese punto entre dos cuadros.

*Consistencia de Brillo*: Se asume que el brillo (intensidad) de un
punto en la imagen permanece constante entre cuadros consecutivos, lo
que permite relacionar los puntos en diferentes cuadros.

*Restricciones Espaciales y Temporales*: Se considera que los puntos
vecinos en una imagen tienden a tener movimientos similares, y este
movimiento cambia suavemente a lo largo del tiempo.

*Métodos para Calcular el Flujo Óptico*: Métodos Basados en Gradientes:
Utilizan las variaciones del brillo y los gradientes de la imagen para
calcular el movimiento. Un ejemplo es el algoritmo de Lucas-Kanade,
que asume que el flujo óptico es esencialmente constante en una
pequeña ventana de la imagen.

*Métodos Basados en Bloques*: Comparan bloques (pequeñas áreas) de un
cuadro con los del cuadro siguiente, buscando el bloque que mejor se
ajuste. Esto se hace por ejemplo en la técnica de coincidencia de
bloques.

*Métodos Basados en Características*: Identifican características
distintivas en las imágenes (como esquinas o bordes) y rastrean cómo
se mueven estas características entre los cuadros.

*Métodos Basados en Aprendizaje Profundo*: Utilizan redes neuronales
para aprender y predecir el movimiento en secuencias de imágenes.

*Aplicaciones del Flujo Óptico*:
*Seguimiento de Objetos*: Rastrear el movimiento de objetos en videos.
*Estabilización de Video*: Corregir la sacudida en las grabaciones de video.
*Reconstrucción de Escenas 3D*: Ayuda a entender la estructura tridimensional del entorno.
*Análisis de Movimiento*: En deportes o medicina para analizar movimientos del cuerpo humano.

*Limitaciones:* 
Sensible a cambios de iluminación.  No funciona bien en
escenas con mucho movimiento o sin texturas.  
La asunción de consistencia de brillo no siempre es válida.  
El flujo óptico es una herramienta poderosa en visión por computadora, pero su precisión y
eficacia dependen en gran medida del método específico utilizado y de
las características de la escena que se está analizando.
  #+BEGIN_SRC python :results output
import numpy as np 
import cv2 as cv

cap = cv.VideoCapture(0)


lkparm =dict(winSize=(15,15), maxLevel=2,
             criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03)) 


_, vframe = cap.read()
vgris = cv.cvtColor(vframe, cv.COLOR_BGR2GRAY)
p0 = np.array([(100,100), (200,100), (300,100), (400,100)])
p0 = np.float32(p0[:, np.newaxis, :])

mask = np.zeros_like(vframe)
cad =''

while True:
    _, frame = cap.read()
    fgris = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    p1, st, err = cv.calcOpticalFlowPyrLK(vgris, fgris, p0, None, **lkparm) 

    if p1 is None:
        vgris = cv.cvtColor(vframe, cv.COLOR_BGR2GRAY)
        p0 = np.array([(100,100), (200,100), (300,100), (400,100) ])
        p0 = np.float32(p0[:, np.newaxis, :])
        mask = np.zeros_like(vframe)
        cv.imshow('ventana', frame)
    else:
        bp1 = p1[st ==1]
        bp0 = p0[st ==1]
        
        for i, (nv, vj) in enumerate(zip(bp1, bp0)):
            a, b = (int(x) for x in nv.ravel())
            c, d = (int(x) for x in vj.ravel())
            dist = np.linalg.norm(nv.ravel() - vj.ravel())

            #print(i, dist)
            
            if i == 0 and dist > 30 :
                cad = cad + '0' 
            elif i == 1 and dist > 30 :
                cad = cad + '1' 
            elif i == 2 and dist > 30 :    
                print('2', dist)
                cad = cad + '2' 
            elif i== 3 and dist > 30 :
                cad= cad+'3' 
           
            frame = cv.putText(frame, cad, (50,50),
                               cv.FONT_HERSHEY_SIMPLEX, 1 , (255,0,0),2, cv.LINE_AA)    
            frame = cv.putText(frame, str(i), (c,d),
                               cv.FONT_HERSHEY_SIMPLEX, 1 , (255,0,0),2, cv.LINE_AA)    
            
            frame = cv.line(frame, (c,d), (a,b), (0,0,255), 2)
            frame = cv.circle(frame, (c,d), 2, (255,0,0),-1)
            frame = cv.circle(frame, (a,b), 3, (0,255,0),-1)
        cv.imshow('ventana', frame)

        vgris = fgris.copy()

        if(cv.waitKey(1) & 0xff) == 27:
            break

cap.release
cv.destroyAllWindows()
  #+END_SRC

#+RESULTS:
#+begin_example
2 120.083694
2 120.879524
2 140.26334
2 186.67227
2 137.3906
2 62.329464
2 64.929184
2 69.902626
2 54.96874
2 52.69248
2 60.124416
2 63.391445
2 89.591415
2 56.381046
2 62.464
2 43.80231
2 39.35062
2 49.12454
2 46.783375
2 38.045883
2 33.4635
2 31.740929
2 57.095345
2 41.95649
2 82.12661
2 58.450493
2 50.61094
2 56.976192
2 52.000736
2 87.50451
2 53.20077
2 67.31267
2 69.54097
2 63.143032
2 35.948177
2 95.95501
2 74.23872
2 76.64836
2 68.564
2 87.3111
2 74.414635
2 72.70697
2 61.554096
#+end_example


*** Vídeo

**** Cargar vídeo simple opencv 
  #+BEGIN_SRC python :results output

import cv2 as cv 

cap = cv.VideoCapture(0)
while(True):
    ret, img = cap.read()
    if ret == True:
        cv.imshow('video', img)
        k =cv.waitKey(1) & 0xFF
        if k == 27 :
            break
    else:
        break
cap.release()
cv.destroyAllWindows()
  #+END_SRC

 #+RESULTS:

**** División de canales de color en vídeo
 #+BEGIN_SRC python
import cv2 as cv 
import numpy as np
cap = cv.VideoCapture(0)
while(True):
    ret, img = cap.read()
    if ret == True:
        img2 = np.zeros(img.shape[:2], dtype=np.uint8)
        b,g,r =cv.split(img)
        rb=cv.merge([g,r,b])
        rg=cv.merge([r,g,b])
        rr=cv.merge([b,r,r])
        #imgGris = cv.cvtColor(img, cv.COLOR_BGR2GRAY)        
        cv.imshow('b',rb)
        cv.imshow('g',rg)
        cv.imshow('r',rr)
        cv.imshow('video', img)
        #cv.imshow('videogris', imgGris)
        k =cv.waitKey(1) & 0xFF
        if k == 27 :
            break
    else:
        break
cap.release()
cv.destroyAllWindows()
 #+END_SRC

**** Seguimiento por color 

#+BEGIN_SRC python
import cv2 as cv 

cap = cv.VideoCapture(0)
while(True):
    ret, img = cap.read()
    if ret == True:
        #cv.imshow('video', img)
        imghsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
        ubb=(100,100, 100)
        uba=(130, 255,255)
        mask = cv.inRange(imghsv, ubb, uba)
        res = cv.bitwise_and(img, img, mask=mask)
        cv.imshow('resultado', res)
        cv.imshow('hsv', imghsv)
        cv.imshow('mask', mask)
        
        k =cv.waitKey(1) & 0xFF
        if k == 27 :
            break
    else:
        break
cap.release()
cv.destroyAllWindows()




#+END_SRC

#+RESULTS:
: None




*** Haarcascades 
Los Haar Cascades son una técnica utilizada en el campo de la visión
por computadora para la detección de objetos. Fueron introducidos por
Paul Viola y Michael Jones en su artículo seminal "Rapid Object
Detection using a Boosted Cascade of Simple Features" en 2001. Esta
técnica es particularmente conocida por su eficacia en la detección de
rostros, aunque puede ser utilizada para detectar otros tipos de
objetos.

#+startup: inlineimages
#+ATTR_LATEX: :width 0.3\textwidth
[[file:img/cascade.png]]

**** Conceptos Clave: 
Características de Haar: Son patrones visuales
 simples que se pueden calcular rápidamente en una imagen. Estas
 características se asemejan a pequeñas versiones de núcleos de wavelet
 de Haar y son utilizadas para capturar la presencia de bordes, cambios
 de textura, y otras propiedades visuales.

 
**** Imágenes Integrales: 
Para acelerar el cálculo de las características
 de Haar, se utiliza un concepto llamado imagen integral. Una imagen
 integral permite calcular la suma de los valores de los píxeles en
 cualquier área rectangular de la imagen en tiempo constante.

****  Adaboost: 
Es un método de aprendizaje automático utilizado para
 mejorar la eficiencia de la detección. Selecciona un pequeño número
 de características críticas de un conjunto más grande y construye
 clasificadores "débiles". Luego, estos se combinan en un clasificador
 más fuerte y eficiente.

****  Cascadas: 
En lugar de aplicar todas las características a una ventana de la
imagen, se organizan en una secuencia de etapas (cascadas). Cada etapa
tiene su propio clasificador (hecho con Adaboost) y solo pasa las
ventanas de la imagen que parecen prometedoras. Esto reduce
significativamente el tiempo de cálculo, ya que muchas ventanas no
pasan las primeras etapas.

 *Proceso de Detección*: 
Pre-procesamiento: Se convierte la imagen en
 escala de grises y se crea su imagen integral.

 *Aplicación de las Características*: Se desplaza una ventana sobre la
 imagen, y en cada posición, se calculan las características de Haar.

 *Clasificación en Cascada*: Cada ventana es evaluada a través de la
 cascada de clasificadores. Si una ventana falla en alguna etapa, se
 descarta. Si pasa todas las etapas, se considera como una detección.

 *Post-procesamiento*: Finalmente, se pueden aplicar técnicas como la
 supresión de no máximos para reducir falsos positivos y mejorar la
 precisión.

 *Aplicaciones*: Detección de rostros en imágenes y videos.  Detección
 de peatones u otros objetos en sistemas de vigilancia.  Aplicaciones
 de realidad aumentada.  Es importante mencionar que, aunque los Haar
 Cascades fueron revolucionarios en su momento, han sido superados en
 precisión y velocidad por técnicas más modernas de aprendizaje
 profundo. Sin embargo, siguen siendo utilizados debido a su
 simplicidad y bajo requerimiento de recursos computacionales.

**** Ejemplo de un Haarcascade

https://github.com/opencv/opencv/tree/master/data/haarcascades

https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html

https://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html

https://amin-ahmadi.com/cascade-trainer-gui/
#+BEGIN_SRC python
import numpy as np
import cv2 as cv
import math 

rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)
i = 0  
while True:
    ret, frame = cap.read()
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gray, 1.3, 5)
    for(x, y, w, h) in rostros:
       #frame = cv.rectangle(frame, (x,y), (x+w, y+h), (0, 255, 0), 2)
       frame2 = frame[ y:y+h, x:x+w]
        #frame3 = frame[x+30:x+w-30, y+30:y+h-30]
       frame2 = cv.resize(frame2, (100, 100), interpolation=cv.INTER_AREA)
       cv.imwrite('/home/likcos/pruebacaras/juan/juan'+str(i)+'.jpg', frame2)
       cv.imshow('rostror', frame2)
    cv.imshow('rostros', frame)
    i = i+1
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
import cv2 as cv 

rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)

while True:
    ret, img = cap.read()
    gris = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gris, 1.3, 5)
    for(x,y,w,h) in rostros:
        res = int((w+h)/8)
        img = cv.rectangle(img, (x,y), (x+w, y+h), (234, 23,23), 2)
        img = cv.rectangle(img, (x,int(y+h/2)), (x+w, y+h), (0,255,0),5 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 21, (0, 0, 0), 2 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 21, (0, 0, 0), 2 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 20, (255, 255, 255), -1 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 20, (255, 255, 255), -1 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 5, (0, 0, 255), -1 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 5, (0, 0, 255), -1 )

    cv.imshow('img', img)
    if cv.waitKey(1)== ord('q'):
        break
    
cap.release
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None

* Reconocimiento de Personas

** Eigenfaces 

Un Eigenface (en español cara propia) es el nombre dado a un conjunto
de vectores propios cuando se utiliza en el problema de visión
artificial del reconocimiento de rostros humanos. Sirovich y Kirby
desarrollaron el enfoque de usar caras propias para el reconocimiento
y lo usaron Matthew Turk y Alex Pentland en la clasificación de
caras. Los vectores propios se derivan de la matriz de covarianza de
la distribución de probabilidad sobre el espacio vectorial de alta
dimensión de imágenes de rostros. Las caras propias forman un conjunto
base de todas las imágenes utilizadas para construir la matriz de
covarianza. Esto produce una reducción de la dimensión al permitir que
el conjunto más pequeño de imágenes base represente las imágenes de
entrenamiento originales. La clasificación se puede lograr comparando
cómo se representan las caras por el conjunto base.

 *Generación*
 Se puede generar un conjunto de caras propias mediante la realización
 de un proceso matemático llamado análisis de componentes principales
 (PCA) en un gran conjunto de imágenes que representan diferentes
 rostros humanos. De manera informal, las caras propias pueden
 considerarse un conjunto de "ingredientes faciales estandarizados",
 derivados del análisis estadístico de muchas imágenes de
 rostros. Cualquier rostro humano puede considerarse una combinación
 de estos rostros estándar. Por ejemplo, la cara de uno podría estar
 compuesta por la cara promedio más el 10 % de la cara propia 1, el 55
 % de la cara propia 2 e incluso el −3 % de la cara
 propia 3. Sorprendentemente, no se necesitan muchas caras propias
 combinadas para lograr una aproximación justa de la mayoría de las
 caras. Además, debido a que la cara de una persona no se registra
 mediante una fotografía digital, sino simplemente como una lista de
 valores (un valor para cada cara propia en la base de datos
 utilizada), se ocupa mucho menos espacio para la cara de cada
 persona.

 Las caras propias que se crean aparecerán como áreas claras y oscuras
 que se organizan en un patrón específico. Este patrón es cómo se
 seleccionan las diferentes características de una cara para
 evaluarlas y puntuarlas. Habrá un patrón para evaluar la simetría, si
 hay algún estilo de vello facial, dónde está la línea del cabello o
 una evaluación del tamaño de la nariz o la boca. Otras caras propias
 tienen patrones que son menos fáciles de identificar, y la imagen de
 la cara propia puede parecerse muy poco a una cara.

 La técnica utilizada en la creación de caras propias y su uso para el
 reconocimiento también se utiliza fuera del reconocimiento facial:
 reconocimiento de escritura a mano, lectura de labios, reconocimiento
 de voz, lenguaje de señas /interpretación de gestos con las manos y
 análisis de imágenes médicas. Por lo tanto, algunos no usan el
 término "eigenface", sino que prefieren usar 'eigenimage'.



#+BEGIN_SRC python :results output
import cv2 as cv 
import numpy as np 
import os
dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
print(np.count_nonzero(np.array(labels)==0)) 

faceRecognizer = cv.face.EigenFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloEigenface.xml')

#+END_SRC

#+RESULTS:
: ['lalo']
: 169

#+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.EigenFaceRecognizer_create()
faceRecognizer.read('laloEigenface.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        #cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] > 2800:
            cv.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv.LINE_AA)
            cv.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv.LINE_AA)
            cv.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2)
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:



** Fisherfaces


 El algoritmo Fisherfaces es una técnica de reconocimiento facial que
 forma parte del campo del aprendizaje automático y la visión por
 computadora. Este algoritmo es una extensión del método de Análisis de
 Componentes Principales (PCA) y fue diseñado específicamente para
 mejorar la capacidad de reconocimiento en situaciones donde la
 iluminación y las expresiones faciales varían significativamente.

 La idea central detrás de Fisherfaces es reducir la dimensionalidad de
 las imágenes faciales manteniendo al mismo tiempo la capacidad de
 distinguir entre diferentes clases (es decir, diferentes
 personas). Esto se logra mediante el Análisis Discriminante Lineal
 (LDA), que es la base del método Fisherfaces. 

 Preprocesamiento: Las imágenes faciales se normalizan en términos de
 tamaño, orientación e iluminación.

 *Análisis de Componentes Principales (PCA)*: Se realiza PCA para reducir
 la dimensionalidad de los datos. PCA identifica las direcciones en las
 que los datos varían más y proyecta los datos en un espacio de menor
 dimensión preservando estas variaciones principales.

 *Análisis Discriminante Lineal (LDA)*: Después de aplicar PCA, se
 utiliza LDA para encontrar las combinaciones lineales de
 características que mejor separan las diferentes clases (diferentes
 personas). Mientras que PCA busca direcciones que maximizan la
 varianza en los datos, LDA busca maximizar la separación entre las
 diferentes clases.

 *Proyección y Clasificación*: Las imágenes se proyectan en el espacio de
 características obtenido por PCA y LDA. Luego, se utiliza un
 clasificador (como k-NN o máquinas de vectores de soporte) para
 identificar a qué clase (persona) pertenece cada imagen proyectada
 basándose en las características extraídas.

 El algoritmo Fisherfaces es particularmente efectivo en situaciones
 donde las variaciones entre las imágenes de una misma clase (por
 ejemplo, las diferentes expresiones faciales de una persona) son
 menores en comparación con las variaciones entre clases diferentes
 (diferentes personas). Esto lo hace robusto frente a cambios en la
 iluminación y las expresiones faciales, siendo una técnica popular en
 aplicaciones de reconocimiento facial.

#+CAPTION: Script para leer un dataset y generar el entrenamiento con FisherFaces
 #+BEGIN_SRC python ::results
import cv2 as cv 
import numpy as np 
import os

dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
#print(np.count_nonzero(np.array(labels)==0)) 
faceRecognizer = cv.face.FisherFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloFisherFace.xml')


 #+END_SRC

 #+RESULTS:
 : None

 #+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.FisherFaceRecognizer_create()
faceRecognizer.read('laloFisherFace.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] < 500:
            cv2.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv2.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2)
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()



 #+END_SRC

 #+RESULTS:
 : None



** LBPH
El LBPH es un enfoque simple y efectivo para el reconocimiento
facial. A diferencia de otros métodos que operan en todo el rostro, el
LBPH trabaja examinando características locales. Su popularidad se
debe a su simplicidad, velocidad y buen rendimiento, incluso en
condiciones de iluminación desafiantes. Aquí está cómo funciona:

División de la Imagen en Celdas: La imagen del rostro se divide en
pequeñas regiones o celdas.

*Calculo de Patrones Binarios Locales (LBP):* Para cada píxel en una
región, se compara su intensidad con las de sus vecinos (generalmente
8 vecinos circundantes). Si la intensidad del vecino es mayor o igual
que el píxel central, se asigna un 1, de lo contrario un 0. Esto
genera un número binario de 8 dígitos (o un número decimal después de
la conversión) para cada píxel.

*Histogramas:* Se calcula un histograma de estas etiquetas LBP para cada
celda. Los histogramas cuentan la frecuencia de cada número obtenido
en el paso anterior dentro de la celda.

*Concatenación de Histogramas:* Los histogramas de todas las celdas se
concatenan en un solo vector de características. Este vector describe
las características locales de la imagen de la cara.

*Reconocimiento:* Para reconocer un rostro desconocido, se calcula su
vector de características LBPH y se compara con los vectores de
características de las caras conocidas (generalmente usando una medida
de distancia, como la distancia euclidiana). La imagen desconocida se
identifica como la clase (es decir, la persona) cuyo vector de
características conocido sea más cercano al del rostro desconocido.

El LBPH es eficaz en diversas condiciones y no requiere un
preprocesamiento tan intenso como otros métodos de reconocimiento
facial. Puede manejar variaciones en iluminación y expresión facial
bastante bien. Además, su implementación es relativamente sencilla, lo
que lo hace popular para aplicaciones en tiempo real y sistemas
embebidos.

#+BEGIN_SRC python
import cv2 as cv 
import numpy as np 
import os

dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
#print(np.count_nonzero(np.array(labels)==0)) 
faceRecognizer = cv.face.LBPHFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloLBPHFace.xml')

#+END_SRC

#+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.LBPHFaceRecognizer_create()
faceRecognizer.read('laloLBPHFace.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] < 70:
            cv2.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv2.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2) 
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()


#+END_SRC


* Redes Neuronales Convolucionales 

** ¿Cómo funcionan las Convolutional Neural Networks?


La Red Neuronal Convolucional (CNN) es una forma avanzada de Red
Neuronal Artificial diseñada para el procesamiento de imágenes. Emula
la manera en que el cortex visual humano procesa la información
visual, lo que le permite identificar y clasificar distintas
características en las imágenes. Esta capacidad la convierte en una
herramienta eficaz para la "visión" computarizada y el reconocimiento
de objetos.

Las CNN están compuestas por múltiples capas ocultas, cada una
especializada y jerarquizada. Las capas iniciales suelen detectar
elementos simples como líneas y curvas. A medida que la información
avanza a través de la red, las capas subsiguientes procesan aspectos
más complejos, como formas específicas y patrones. Esta progresión
permite a las capas más profundas reconocer objetos más complejos,
como rostros o siluetas animales.

Para que una CNN aprenda a identificar una amplia gama de objetos en
imágenes, es crucial entrenarla con un extenso conjunto de datos. Por
ejemplo, para el reconocimiento de gatos, se necesitarían miles de
imágenes que muestren variaciones en color, raza, postura y
entorno. Este entrenamiento supervisado permite a la red aprender las
características distintivas de cada objeto y generalizar esta
comprensión para reconocer nuevas imágenes de manera efectiva. Así,
una CNN bien entrenada puede distinguir un gato independientemente de
su posición, color o tamaño, adaptándose a una amplia gama de
situaciones visuales.






* Desafíos y Consideraciones Éticas
  - Desafíos Técnicos: Precisión, grandes conjuntos de datos, computación intensiva.
  - Cuestiones de Privacidad: Preocupaciones sobre reconocimiento facial y vigilancia.
  - Futuro de la Visión por Computadora: Impacto en la sociedad y desarrollo continuo.

* Conclusión y Futuro de la Visión por Computadora
  - Resumen: Repaso de los puntos clave.
  - Futuras Tendencias: Inteligencia artificial, aprendizaje profundo.
  - Preguntas y Discusión: Invitación a participar.


